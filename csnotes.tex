\documentclass[10pt]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[OT1]{fontenc}
\usepackage{amsfonts, amsmath, amsthm, amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[margin=1in]{geometry}
\usepackage{xcolor}
\newcounter{countCode}
\newtheorem{theorem}{Theorem}
\lstnewenvironment{code} [1][caption=Ponme caption, label=default]{%
	\renewcommand*{\lstlistingname}{Listado} 
	\setcounter{lstlisting}{\value{countCode}} 
	\lstset{ %
	language=java,
	basicstyle=\ttfamily\footnotesize,       % the size of the fonts that are used for the code
	numbers=left,                   % where to put the line-numbers
	numberstyle=\sc,      % the size of the fonts that are used for the line-numbers
	stepnumber=1,                   % the step between two line-numbers. 
	numbersep=5pt,                 % how far the line-numbers are from the code
	numberstyle=\color{red!50!blue},
    	backgroundcolor=\color{lightgray!20},
	rulecolor=\color{blue},
	keywordstyle=\color{red}\bfseries,
	showspaces=false,               % show spaces adding particular underscores
	showstringspaces=false,         % underline spaces within strings
	showtabs=false,                 % show tabs within strings adding particular underscores
	frame=single,                   % adds a frame around the code
	framexleftmargin=0mm,
	numberblanklines=false,
	xleftmargin=5pt,
	breaklines=true,
	breakatwhitespace=true,
	breakautoindent=true,
	captionpos=t,
	texcl=true,
	tabsize=2,                      % sets default tabsize to 3 spaces
	extendedchars=true,
	inputencoding=utf8, 
	escapechar=\%,
	morekeywords={print, println, size, background, strokeWeight, fill, line, rect, ellipse, triangle, arc, save, PI, HALF_PI, QUARTER_PI, TAU, TWO_PI, width, height,},
	emph=[1]{print,println,}, emphstyle=[1]{\color{blue}}, % Mis palabras clave.
	emph=[2]{width,height,}, emphstyle=[2]{\bf\color{violet}}, % Mis palabras clave.
	emph=[3]{PI, HALF_PI, QUARTER_PI, TAU, TWO_PI}, emphstyle=[3]\color{orange!50!violet}, % Mis palabras clave.
	emph=[4]{line, rect, ellipse, triangle, arc,}, emphstyle=[4]\color{green!70!black}, % Mis palabras clave.
	%emph=[5]{size, background, strokeWeight, fill,}, emphstyle=[5]{\tt \color{red!30!blue}}, % Mis palabras clave.
	%emph={[2]sqrt,baset}, emphstyle={[2]\color{blue}}, % f(sqrt(2)), sqrt a nivel 2 se pondrá azul
	#1}}{\addtocounter{countCode}{1}}



\title{Computer Science Lecture Notes}
\author{Job Hernandez Lara}
\date{\today}
\begin{document}
\maketitle \tableofcontents 


\begin{abstract}
This document represents lecture notes for a set of computer science textbooks that cover the core of the computer science degree curriculum at the University of Washington in Seattle. The fundamentals of the curriculum include: CSE 143 Computer Programming 2, CSE 311, 312 Discrete math, CSE 331 Software Design and Implementation, Introduction to Algorithms, and CSE 351 Hardware/Software Interface whereas the core of the University of Washington CS curriculum include about 8 upper level courses; but in these lecture notes I will cover compilers, and operating system in addition to the fundamentals.
\end{abstract}

\section{Discrete Math}

\subsection{Constructing Direct Proofs}
A conditional statement is statement written as $P\implies Q$ where $P$ is the hypothesis and $ Q $ is the conclusion. Intuitively, $P \implies Q$ means that $Q$ is true whenever $P$ is true; in other words, if $P$ is true then it follows necessarily that $Q$ is true. Here is the truth table for conditional statements:

\begin{center}
    \[
\begin{array}{r|c|l} 
P & Q & P \implies Q\\
\hline
T & T & T\\
T & F & F\\
F & T & T\\
F & F & T\\
\end{array}
\]
\end{center}
A direct proof is a type of proof in which the mathematician demonstrates that one mathematical statement follows logically from definitions and previously proven statements. To prove that a conditional statement $P \implies Q$ is true we only need to prove the $Q$ is true whenever $P$ is true. Why? Because $P\implies Q$ is true whenever $P$ is false. If you take a look at the truth table for conditional statements you will notice that $P \implies Q$ is false when $P$ is true and $Q$ is false. So, by demonstrating that $Q$ is true whenever $P$ is true we can prove the statement because you are guaranteed that the statement is true. There is a technique for proving by this method called the \textbf{know-show-table.} In this technique we work forward from the hypothesis and backwards from the conclusion and we try to connect the two by building a chain of reasoning. You start with the conclusion and ask "under what conditions is the conclusion true?" and then work backwards. Suppose  we are given the statement: \textit{If $x$ and $y$ are odd integers, then $x \cdot y$ is an odd integer.} Here is the table for this statement:

\begin{center}
    \[
\begin{array}{|r|c|l|} \hline
Step & Know & Reason\\
\hline
P & x\hspace{.1cm} \text{and} y \text{are odd integers} & \text{Hypothesis}\\
P1 & \text{There exists integers $m$ and $n$ such that $x = 2m + 1$ and $y = 2n + 1$}  & \text{Definition of an odd integer.} \\
P2 & xy = (2m + 1)(2n + 1) & \text{Substitution}\\
P3 & xy = 4mn + 2m + 2n + 1 & \text{Algebra} \\
P4 & xy = 2(2mn + m + n) + 1 & \text{Algebra}\\
P5 & \text{$2mn + m + n$ is an integer} & \text{Closure properties of the integers}\\
Q1 & \text{There exists an integer $q$ such that $xy = 2q + 1.$}  & \text{Use $q = (2mn + m + n)$}\\
Q & \text{$ x \cdot y$ is an odd integer.} & \text{Definition of an odd integer}\\
Step & Show & Reason
\end{array}
\]
\end{center}

In the above \textbf{know-show-table} we ask, by working backwards (i.e from the conclusion), we ask "How do we prove that an integer is odd?" and then we continue asking the same question until we can connect it with the hypothesis.

\section{Introduction to Algorithms}

\section{Software Construction}

\section{Hardware/Software interface}

\subsection{Representing and Manipulating Information, chap 2}
Bytes are the smallest addressable unit of memory; a byte is 8 bits. In other words the byte is the fundamental unit of information. The processor of your computer only processes bytes; in fact, consider the following quote from the book "Computer Systems - A Programmer's Perspective": "A fundamental concept of computer systems is that a program, from the perspective of the machine, is a sequence of bytes; moreover, another key idea is that bytes are the unit with which machines communicate with one another through networks. A machine level program interacts with the virtual memory which is a large array of bytes. The machine level program does not interact directly with the physical memory. Every byte of memory has an address - the set of all addresses is called the virtual address space. Interestingly, a multi-byte object is stored as a contiguous sequence of  bytes; for example, in 64 bit architecture an integer in C is 4 bytes; if the address of the given int is 0x100, the given int will be stored in memory locations 0x100, 0x101, 0x102, and 0x103.

\subsection{Machine Level Representation, chap 3}
It is important to understand machine level representation because it will enable you to analyze how efficient your code is. By understanding how your code will be compiled to assembly code you will have a much better understanding of the performance of your program. When you are working with assembly code you are directly manipulating the 16 CPU registers; you are also interacting with the program counter. Now lets explore how different high level programming constructs are represented in assembly. Lets start with if statements. Consider this C program: 

\begin{lstlisting}[language=C]
long lt_cnt = 0;
long ge_cnt = 0;
long absdiff_se(long x, long y)
{
	long result;
	if (x < y) {
		lt_cnt++;
		result = y - x;
	}
	else {
		ge_cnt++;
		result = x - y;
	}
	return result;
}

\end{lstlisting} 
The above C code gets compiled to the following x86-64 assembly through compiler passes. The assembly that the above code generates is this: 

\begin{lstlisting}
long absdiff_se(long x, long y)
x in %rdi, y in %rsi
	absdiff_se:
		cmpq	%rsi, %rdi	Compare x:y
		jge	.L2		If >= goto x_ge_y
		addq	$1,lt_cnt(%rip)	lt_cnt++
		movq	%rsi, %rax
		subq	%rdi, %rax	result = y - x
		ret			Return
	.L2:			  x_ge_y:
		addq $1, ge_cnt(%rip)	ge_cnt++
		movq %rdi, %rax
		subq %rsi, %rax		result = x - y
		ret			Return

\end{lstlisting}
Now, lets consider the assembly generated by writing a while loop: 

\begin{lstlisting}[language=C]
long fact_while(long n)
{
	long result = 1;
	while (n > 1) {
		result *= n;
		n = n-1;
	}
	return result;
}

\end{lstlisting}
The above code generates the following assembly:

\begin{lstlisting}
long fact_while(long n)
	n in %rdi
	fact_while:
	  movl	$1, %eax	Set result = 1
	  jmp	.L5		Goto test
	.L6:		    loop:
	  imulq	%rdi, %rax	 Compute result *= n
	  subq	$1, %rdi	Decrement n
	.L5:		    test:
	  cmpq	$1, %rdi	 Compare n:1
	  jg	.L6		If >, goto loop
	  rep; ret		Return
\end{lstlisting}
Before saying something about how procedures get compiled to assembly code, I would like to say something about the stack. The stack is dynamic -- it grows as functions get called and the space that was allocated will be freed as the function returns. Stack memory behaves just like the stack data structure -- i.e., last-in, first out. 
Here is the example in C:

\begin{lstlisting}[language=C]
long swap_add(long *xp, long *yp)
{
	long x = *xp;
	long y = *yp;
	*xp = y;
	*yp = x;
	return x + y;
}
long caller()
{
	long arg1 = 534;
	long arg2 = 1057;
	long sum = swap_add(&arg1, &arg2);
	long diff = arg1 - arg2;
	return sum * diff;
}
\end{lstlisting}
The above code gets compiled to the following assembly: 

\begin{lstlisting}
long caller()
	caller:
	subq	$16, %rsp	Allocate 16 bytes for stack frame
	movq	$534, (%rsp)	Store 534 in arg1
	movq	$1057, 8(%rsp)	Store 1057 in arg2
	leaq	8(%rsp), %rsi	Compute &arg2 as second argument
	movq	%rsp, %rdi	Compute &arg1 as first argument
	call	swap_add	Call swap_add(&arg1, &arg2)
	movq	(%rsp), %rdx	Get arg1
	subq	8(%rsp), %rdx	Compute diff = arg1 - arg2
	imulq	%rdx, %rax	Compute sum * diff
	addq	$16, %rsp	Deallocate stack frame
	ret Return
\end{lstlisting}

\subsection{The Memory Hierarchy, chap 6}
Memory can be seen as layers of increasingly slower memory layers; at the top of the memory hierarchy you have registers, and then cache layers, then the memory layer, and then the disk, and the disk in remote network servers. Each level k, starting from L0, uses level k+1 to retrieve data. So, interestingly the local disk retrieves from the disk in the remote servers; this was insightful for me because it connected network applications such web apps to the rest of the system. With respect to clock cycles, the processor takes 0, 4, hundreds of cycles to read from registers, cache, and memory respectively. This is why register allocation is so important in compilers and why register allocation improves the compiler performance -- i.e., it is more efficient to retrieve from registers than memory. As programmers, if we want to write efficient programs we should aim at writing programs with good locality. A program has good temporal locality if the instructions in a loop execute during each iteration of the loop whereas spatial locality refers when memory is accessed sequentially. Consider the following program.

\begin{lstlisting}[language=C]
int sumvec(int v[N])
	{
	int i, sum = 0;

	for (i = 0; i < N; i++)
		sum += v[i];
	return sum;
}
\end{lstlisting}
`sumvec` has good temporal locality because the variable `sum` is referenced during each iteration of the loop but since `sum` is a scalar theres no spatial locality. On the other hand since "v" is read (fetched) sequentially during all iteration of the loop it has good spatial locality but it has poor temporal locality because each element of "v" is accessed only once. A given function is said to have good locality if the variables have either temporal locality or spatial locality. Another quality of `sumvec` is that it has a stride-1 reference pattern. A function has a stride-1 reference pattern if each element of an array is visited sequentially. If the kth element of an array is visited then it has a stride-k reference pattern. The higher the stride the less spatial locality. Now lets consider a 2-dimensional array.

\begin{lstlisting}[language=C]
int sumarrayrows(int a[M][N])
{
	int i, j, sum = 0;
	
	for (i = 0; i < M; i++)
		for (j = 0; j < N; j++)
			sum += a[i] [j];
       return sum;
}
\end{lstlisting}
`sumarrayrows` enjoys good spatial locality because it references the array in row-major order; that is, it references one row at a time which is how C arrays are laid out in memory. `sumarrayrows` has a stride-1 reference pattern. You probably have heard about loop interchange, a compiler optimization. Here is an example of how interchanging gives rise to worst performance.

\begin{lstlisting}[language=C]
int sumarraycols(int a[M][N])
{
	int i, j, sum = 0 ;

	for (j = 0; j < N; j++) {
          for (i = 0; i < M; i++) {
	       sum += a[i] [j];
        }
    }
    return sum;
}  
\end{lstlisting}
This function above has poor spatial locality because it references the array column by column so it has a stride-N reference pattern. As to what cache hits and cache misses mean, a cache hit is when a program looks for an item d in level k+1 but finds d in level k. Conversely, a cache miss is when d is not found in level k but instead is found in level k+1. Recall that higher levels in the memory hierarchy are faster. Here is a quote from "Computer Systems, A Programmer's Perspective": "Programs with better locality will tend to have lower miss rates, and programs with lower miss rates will tend to run faster than programs with higher miss rates. Thus, good programmers should always try to write code that is cache friendly, in the sense that it has good locality." With respect to how compilers work, any respectable optimizing compiler will cache local variables with good temporal locality in the register file. "In general, if a cache has a block size of B bytes, then a stride-k reference pattern (where k is expressed in words) results in an average of min (1, (word size × k)/B) misses per loop iteration. This is minimized for k = 1, so the stride-1 references to v are indeed cache friendly."

\subsection{Exceptional Control Flow, chap 8}
I was familiar with this chapter because I have basic OS knowledge but a few things stood out. Firstly, the processor has control flow. From the time the computer gets turned on to the time it shuts down the processor fetches instructions and executes them one at a time. This sequence is the control flow. The control flow can be altered by the program state - i.e., jumps, branches, call, return. But other changes to the control flow react to system changes such as data arriving from disk or network adapter. This type of change in the control flow due to system changes is known as exceptional control flow; there is two mechanisms for exceptional control flow: exceptions and higher level mechanisms such as context switch. Exceptions happen in response to a significant change in the processors  state -- e.g., virtual memory page fault occurs, an arithmetic overflow occurs, or an instruction attempts a divide by zero. Control flow passes from one process to another via context switch.

\section{Compilers}
Compilers can be seen as a composition of a front-end, optimizer, and back-end. In these notes I will skip the optimizer. At a high level the front-end takes as input the source language and produces an \textit{abstract syntax tree}. An AST is a tree whose nodes represent each component of the given expression. The AST is then converted into a graph or is a lower AST. Each pass of the of the compiler lowers the AST closer to the assembly code. As to the back-end, there's instruction selection and register allocation. Instruction selection consists of taking a low AST as input and translating it into an even lower AST consisting of the target assembly code instructions -- e.g. x86-64. On the other hand register allocation consists of using registers to allocate the variables of the program but since there is very little registers you need to implement graph algorithms to use the CPUs registers for all the variables. Register allocation makes the compiler more efficient because the processor takes 0 cycles to fetch from registers as oppose to fetching from memory which take hundreds of cycles. Obviously, what I have written above is a very simplified explanation but in this section I am going to include some code. Consider compiling a very small subset of Scheme. Suppose we are compiling \textit{let} expressions and simple arithmetic. Here is a complete parser:

\begin{lstlisting}[language=python]
def parse_tree_to_ast(tree):
    """
    converts the parse tree into an abstract synstax tree.

    @param tree: the parse tree
    @returns: abstract syntax tree
    """
    match tree:
        case x if isinstance(x, lark.tree.Tree):
            if tree.data == 'start':
                return [make_parse_tree(x) for x in tree.children]
            
            elif tree.data == 'let':
                let_exps = [make_parse_tree(x) for x in tree.children]
                if len(let_exps) == 2:
                    let_bindings = let_exps[0]
                    let_body = let_exps[1]
                    return Let(let_bindings, let_body)
                else:
                    length = len(let_exps)
                    let_bindings = let_exps[:length-1]
                    let_body = let_exps[length-1]
                    return Let(let_bindings, let_body)
            elif tree.data == 'binding':
                return Binding([make_parse_tree(x) for x in tree.children])
            else:
                return List([make_parse_tree(x) for x in tree.children])

        case x if isinstance(x, lark.lexer.Token):
            ty = tree.type.lower()
            if ty == 'atom':
                return Atom(tree.value)

            else:
                return Int(tree.value)
    
\end{lstlisting}
The above program takes a parse tree and converts it into an AST whose nodes are represented by Python classes. The generated AST then goes through multiple passes which transform one ast to a lower ast; for example, here is an example of the main passes:

\begin{lstlisting}[language=python]

def uniquify(ast, lets_dict, counter):
    """ 
     given a let expression that is nested this pass ensures that each var is
    unique.

    param: ast
    returns: a uniquified pass ast

    Example:
        (let ((x 2)) (+ (let ((x 5)) x) x))
        -> 
        (let ((x.1 2)) (+ (let ((x.2 5)) x.2) x.1))

    Example 2:
        (let ((x 1)) (+ (let ((x 4)) (+ (let ((x 5)) x) x)) x))
        ->
        (let ((x.1 1)) (+ (let ((x.2 4)) (+ (let ((x.3)) x.3) x.2) x.1))
    
    Example 3:
        (let ((x 1)) (+ (let ((x 4)) (+ (let ((x 5)) (+ (let ((x 6)) x) x)) x)) x))
        ->
        (let ((x.1 1)) (+ (let ((x.2 4)) (+ (let ((x.3 5)) (+ (let ((x.4 6)) x.4) x.3)) x.2)) x.1))
    """
    ...

def remove_complex(ast):
    """
    removes the complex expressions resulting only in atomic expressions.

    @params ast: the ast transformed by the `uniquify` pass
    @returns: ast with atomic expressions

    Example:
        (+ 53 (- 10)) -> (let ((temp.1 (- 10))) (+ 53 temp.1))

    Example 2:
        (let ((x (+ 10 (- 3))) x))
        ->
        (let ((x (let ((temp.1 (- 3))) (+ 10 temp.1))) x)

    """
    ...

def explicate_control(ast, counter, vars, assignments):
    """
    Make order of execution clear.

    @param ast
    @param counter: counter for dictionaries
    @param vars: dict
    @param assignments: dict
    @returns: ast with clear order of exceution

    Example:
       (let ((x.1 2)) (+ (let ((x.2 5)) x.2) x.1))
       ->
       start:
         x.1 = 2
         x.2 = 5
         return x.1 + x.2

    Example 2:
        (let ((x.1 1)) (+ (let ((x.2 4)) (+ (let ((x.3 5)) x.3) x.2) x.1))
        ->
        start:
           x.1 = 1
           x.2 = 4
           x.3 = 5
           return x.1 + x.2 + x.4

    Example 3:
       (let ((temp.1 (- 10))) (+ 53 temp.1))
       ->
       start:
          temp.1 = - 10
          return 53 + temp1

    Example 4:
        (let ((x (let ((temp.1 (- 3))) (+ 10 temp.1))) x)
        ->
        start:
            temp1 = - 3
            x = 10 + temp1
            return x
    Example 5:
         (+ 3 4)
    """

def select_instructions(ast):
    """
    makes the assembly instructions explicit.

    @param ast
    @returns: assembly (x86-64) based ast.

    Example:
        start:
         x.1 = 2
         x.2 = 5
         return x.1 + x.2
        ->
        movq 2, x
        addq 5, x
        retq

    Example 2:
         start:
           x.1 = 1
           x.2 = 4
           x.3 = 5
           return x.1 + x.2 + x.4
        ->
        movq 1, x1
        addq 4, x1
        addq 5, x1
        retq

    """
    ...
    
\end{lstlisting}
As you can see above each pass lowers the ast to a much lower level. Take a look at the examples in each function so you can get an idea of what is doing. Given a Scheme \textit{let} expression you transform it, pass by pass, to a sequence of instructions which resemble how assembly works. As explained above the \textit{select instructions} pass lowers the ast into explicit x86-64 instructions.

\section{Operating Systems}

\subsection{Processes and Threads}
A process is an abstraction for a program in execution; each process has it own address space and its memory is layed out as follows: text section, data section, heap and stack. The text section consists of the executable code, the data section consists of global variables, the stack consists of data storage associated with invocations of functions such as parameters, return address, and the heap is associated with dynamically memory storage such as when you allocate memory in C with malloc. A thread is the execution control center of a given process. There could be multiple threads per process. A thread has its own stack but shares the same address space with other threads. This makes sense because threads need to be aware of the same program.

\subsection{Interprocess Communication}
Often process needs to communicate with other process because they may need to access main memory. This can lead to race conditions. A race condition happens when two or more processes are sharing data (from main memory for instance) and the final result depends on the order of the processes. If one process runs before another process it may change the result because the order is off. How do we avoid race conditions? The answer is mutual exclusion; mutual exclusion dictates that when two or more processes share memory the processes that are not using the shared data are excluded from accessing the data. Mutual exclusion is achieved by semaphores, mutexes. A semaphore is initialized to the number of resources; when a process wants to use a resource it calls the wait() function thereby decrementing the count. When the process releases a resource it performs a signal() thereby incrementing the count. When the count is 0 processes are blocked until it increments again. When a process modifies a semaphore value another process cannot access the same semaphore value -- i.e., cannot access the same resource.

\subsection{Memory Management}
The job of the memory manager is to keep track of the parts of memory that are being used, to allocate memory to processes and to deallocate memory when the processes are done with it. How does the memory manager accomplish this? It does this through an abstraction mechanism known as virtual memory. As discussed above a process is an abstraction for a program; likewise, an address space is abstraction of main memory -- it gives the illusion that each process has its own main memory. The set of addresses that a process can use to address memory is called the address space. A further extension of this is called virtual memory whereby a computer can work with programs that have more memory than what the computer is capable; the basic idea behind main memory is that a process has its own address space which is layed out as pages where a page is a contiguous range of memory addresses. How does the memory manager manage free memory? Free memory is managed by a bitmap; in a bitmap memory is divided by allocation units where each allocation unit is either free or being used. A free unit is denoted by a bit 0 whereas a unit being used is denoted bit 1. When a k unit process asks for memory the memory manager needs to allocate consecutive k 0 bits. A further question to ask now is how is this implemented in C. One thing to keep in mind is the programs refer to memory addresses which programming in assembly makes explicit with the statement `mov 1000, reg`. So, programs generate addresses which are called virtual addresses if a system has virtual memory. Virtual address then get translated to physical memory addresses.

\section{Computer Architecture}
A computer can be seen a hierarchy of layers: digital logic level, microarchitecture level, ISA level and Operating System level. The digital logic level is the lowest level whose circuits, made of transistors, carry out the machine level program of the the microarchitecture level. On the other hand the microarchitecture level consists of a circuit called the Arithmetic Logic Unit and a set of registers that form a memory; The ALU and memory form a data path which data flows. At this level there exists a microprogram that executes the instructions the next level up--namely, the instruction set level. The next level up is the operating system level; the operating system manages the resources.

\subsection{Computer Systems Organization}
A computer is built from processors, memory, and i/o devices.
\subsubsection{Processor}
The processor is comprised of the control unit, the arithmetic logic unit, and a set of registers; the control unit is responsible of fetching instructions from main memory, the ALU
is responsible of doing arithmetic and the registers hold the ALU input. The registers feed into ALU input registers which hold the ALU input while the ALU is performing some computation. There is also ALU output registers which hold the output of the ALU and whose data can be sent to registers again or written to memory. How does the cpu carry out the instructions?

\begin{lstlisting}
1. Fetch the next instruction from memory into the instruction register.
2. Change the program counter to point to the following instruction.
3. Determine the type of instruction just fetched.
4. If the instruction uses a word in memory, determine where it is.
5. Fetch the word, if needed, into a CPU register.
6. Execute the instruction.
7. Go to step 1 to begin executing the following instruction
\end{lstlisting}

Instruction level parallelism is a process whereby the processor executes more instructions per second; it does this by implementing pipelining where more stuff gets done in less processor cycles. Instruction level parallelism is exploited in compilers.

\subsubsection{Main Memory}
The basic unit of memory is the bit; a bit can either be 0 or 1. Main memory is comprised of cells or locations which have addresses. The operating system's virtual memory abstracts all this. Memory is slower than CPU and as a consequence it takes hundreds of cycles to fetch from memory. A solution to this is cache memory which is considerably faster than main memory --- taking the processor only a few cycles to fetch from cache memory.

\subsection{Digitial Logic Level}
\subsubsection{Gates}
Gates are tiny electronic devices that can compute binary functions; some examples include the not, and, or, and xor gates. Interestingly, a signal between 0 and 0.5 volts corresponds to binary 0 and a signal between 1 and 1.5 volts corresponds to binary 1. From these gates bigger circuits are built -- e.g, memory, adders, CPUs. A key question here is what are clocks and how are clocks related to the performance of a given processor? A clock is a circuit that emits a series of pulses with a precise pulse width and precise interval between consecutive pulses. The time interval between two consecutive pulses is called the clock cycle time; pulse frequencies range from 100Mhz and 4Ghz corresponding to 10 nsec to 250 psec.

\subsubsection{Processor Architecture}
How are the operating system and processor architecture related? Y86-64 (simplified x86-64 version) reference memory locations using virtual addresses; virtual addresses then get mapped to physical memory. One question I have is how do virtual addresses get mapped to physical addresses? And how are virtual addresses related to virtual memory? I think the answer to this second question is that virtual memory is an abstraction for main memory; that is, virtual memory provides the illusion that a given program in execution (i.e., process) has its own memory; virtual memory is laid out as locations or cells; each location has an address; the set of all addresses is collectively called the virtual address space. How is virtual memory implemented in the OS?  The instructions, registers that you can access as a programmer is called the programmer-visible-state. This also includes the compiler which generates the assembly code. How is the processor exposed to the programmer? The source and destination of the the `movq` instruction consists of the immediate, registers, or memory; however, only registers and memory are destinations. This reflects the memory hierarchy: registers are faster than memory; remember, from fastest to slowest you have registers, cache, and memory which take respectively 0, 4, hundreds of cycles; and remember, a clock is a circuit that emits pulses with a precise pulse width and a time interval between consecutive pulses. The time interval between two consecutive pulses is called the clock cycle time; pulse frequencies range from 100Mhz and 4Ghz corresponding to 10 nsec to 250 psec. This pulse frequency falls in the range of 4ghz. A lot events happen during a given clock cycle. Gates are combined into combinational circuits that perform a much more complex function; these circuits operate on data words. On the other hand, memory and storage devices are controlled by clocks; another definition of a clock is a signal that determines when new data are to be loaded into the devices. Every instruction is executed through the five stages mentioned above during one clock cycle; here is a quote from \textit{Computer Systems - A Programmers Perspective}: "We are left with just four hardware units that require an explicit control over their sequencing—the program counter, the condition code register, the data memory, and the register file. These are controlled via a single clock signal that triggers the loading of new values into the registers and the writing of values to the random access memories. The program counter is loaded with a new instruction address every clock cycle. The condition code register is loaded only when an integer operation instruction is executed. The data memory is written only when an rmmovq, pushq, or call instruction is executed. The two write ports of the register file allow two program registers to be updated on every cycle, but we can use the special register ID 0xF as a port address to indicate that no write should be performed for this port."

\section{Distributed Systems}
\subsection{The role of processes in distributed systems}
A distributed system is a system whose components are spread across different computers; and example of a distributed system is a web application; a web application is a distributed system because the client and server are situated in different machines but yet the end user perceives a cohesive whole.

Threads are particularly important in distributed systems because without threads processes associated with clients or servers would block; for example, in a multithreaded server a thread can accept requests and another thread can fetch from disk; by doing this a process does not have to wait for another request while it is fetching a file from disk. Servers can be replicated across machines in a given data center. Furthermore, suppose a system is composed of multithreaded client and mutlthreaded server. The multithreaded client requests multiple files for a given page to hide latencies. If the server is replicated, after a load balancer routes the requests each of the replicated servers can serve each request of the client. The fact that the user of google chrome does not know anything about where the server is located or anything else about the server is called distribution transparency --- clients are built with distribution transparency in mind.

\subsection{Communication between processes}
\subsubsection{Remote Procedure Calls}
Remote procedure calls (RPCs) is a synchronous communication mechanism whereby the stub of a procedure is situated on the client and the client sends a request to the server and the server executes this remote procedure. Why do you want to carry out a computation by calling the stub in the client and carrying out the actual computation on a remote server? Distributed systems traditionally involve message passing -- i.e., a send and recieve functions. This mechanism does not support distribution transparency while RPCs do. In a distributed system you want to conceal things about the server--e.g, server replication. By the way, synchronous communication means that when a client sends a request the client has to wait (cannot send another request) until the server responds.

\subsubsection{Message Passing Interface}
MPI is an standardization of message passing -- it is platform independent. MPI is used for transient communication; transient communication means that messages are stored by the communication system only as long as the sending and receiving applications are in execution. In MPI groups of processors have an identifier and each processor has an identifier as well. So, the source and destination of communication is known. In MPI groups can communicate with other groups; there is also process to process communication and there is communication among all the processes in a given group; for example, one node can send messages to other nodes and there could be a sum of messages that are then stored in one node.

\subsubsection{Message Queuing}
Message Queuing communication systems are asynchronous and offer intermediate term storage for messages. Under this model, applications communicate by inserting messages in queues. Message queuing systems integrate existing and new applications into a coherent distributed information system. Message brokers convert incoming messages so they can be understood by the destination server. Examples include RabbitMQ.

\subsection{Fault tolerance, Consistency, and Replication}
In big websites such as Facebook there's the thousands of servers so naturally there will always be something broken. In a distributed system you need to hide this. A big idea here is replication of servers which lead to "High Availability" which means that service continues despite failures. How are multiple servers, fault tolerance, consistency and replication related? Many servers imply constant faults; fault tolerance imply replication; replication imply potential inconsistencies; better consistency imply low performance.


\end{document}
